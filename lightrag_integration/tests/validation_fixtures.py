#!/usr/bin/env python3
"""
Validation Test Fixtures for Clinical Metabolomics Oracle.

This module provides comprehensive validation fixtures for testing biomedical content
accuracy, scientific validity, and clinical relevance of responses generated by
the Clinical Metabolomics Oracle LightRAG integration.

Components:
- BiomedicalContentValidator: Validates biomedical accuracy and consistency
- ClinicalRelevanceValidator: Assesses clinical applicability and safety
- ScientificAccuracyChecker: Verifies scientific facts and relationships
- MetabolomicsKnowledgeValidator: Validates metabolomics-specific content
- ResponseQualityAssessor: Assesses overall response quality and completeness
- ValidationScenarioGenerator: Creates comprehensive validation test scenarios

Author: Claude Code (Anthropic)
Created: August 7, 2025
Version: 1.0.0
"""

import pytest
import random
import json
import time
import re
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from collections import defaultdict
from enum import Enum
import numpy as np


class ValidationLevel(Enum):
    """Validation severity levels."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class ValidationType(Enum):
    """Types of validation checks."""
    BIOMEDICAL_ACCURACY = "biomedical_accuracy"
    CLINICAL_SAFETY = "clinical_safety"
    SCIENTIFIC_VALIDITY = "scientific_validity"
    FACTUAL_CONSISTENCY = "factual_consistency"
    REFERENCE_ACCURACY = "reference_accuracy"
    NUMERICAL_VALIDATION = "numerical_validation"
    TERMINOLOGY_CONSISTENCY = "terminology_consistency"
    LOGICAL_COHERENCE = "logical_coherence"


@dataclass
class ValidationResult:
    """Result of a validation check."""
    validation_id: str
    validation_type: ValidationType
    validation_level: ValidationLevel
    passed: bool
    confidence: float
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    evidence: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    timestamp: float = field(default_factory=time.time)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            'validation_id': self.validation_id,
            'validation_type': self.validation_type.value,
            'validation_level': self.validation_level.value,
            'passed': self.passed,
            'confidence': self.confidence,
            'message': self.message,
            'details': self.details,
            'evidence': self.evidence,
            'suggestions': self.suggestions,
            'timestamp': self.timestamp
        }


@dataclass
class ValidationReport:
    """Comprehensive validation report."""
    report_id: str
    content_analyzed: str
    total_validations: int
    passed_validations: int
    failed_validations: int
    critical_issues: int
    warnings: int
    overall_score: float
    validation_results: List[ValidationResult] = field(default_factory=list)
    summary: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)
    
    @property
    def pass_rate(self) -> float:
        """Calculate validation pass rate."""
        return self.passed_validations / self.total_validations if self.total_validations > 0 else 0.0
    
    @property
    def is_safe_for_clinical_use(self) -> bool:
        """Determine if content is safe for clinical use."""
        return self.critical_issues == 0 and self.pass_rate >= 0.8


class BiomedicalContentValidator:
    """
    Validates biomedical content for accuracy, consistency, and safety.
    """
    
    # Comprehensive biomedical knowledge base for validation
    METABOLITE_KNOWLEDGE_BASE = {
        'glucose': {
            'molecular_formula': 'C6H12O6',
            'molecular_weight': 180.16,
            'normal_range_plasma': (3.9, 6.1),  # mM
            'normal_range_units': 'mM',
            'alternative_units': {'mg/dL': (70, 110)},
            'pathways': ['glycolysis', 'gluconeogenesis', 'pentose_phosphate_pathway'],
            'clinical_significance': ['diabetes', 'hypoglycemia', 'metabolic_syndrome'],
            'analytical_methods': ['enzymatic_assay', 'LC-MS/MS', 'GC-MS'],
            'synonyms': ['d-glucose', 'dextrose', 'blood_sugar'],
            'hmdb_id': 'HMDB0000122',
            'kegg_id': 'C00031'
        },
        'lactate': {
            'molecular_formula': 'C3H6O3',
            'molecular_weight': 90.08,
            'normal_range_plasma': (0.5, 2.2),  # mM
            'normal_range_units': 'mM',
            'alternative_units': {'mg/dL': (4.5, 19.8)},
            'pathways': ['glycolysis', 'cori_cycle'],
            'clinical_significance': ['tissue_hypoxia', 'sepsis', 'heart_failure'],
            'analytical_methods': ['enzymatic_assay', 'LC-MS/MS'],
            'synonyms': ['lactic_acid', 'milk_acid'],
            'hmdb_id': 'HMDB0000190',
            'kegg_id': 'C00186'
        },
        'cholesterol': {
            'molecular_formula': 'C27H46O',
            'molecular_weight': 386.65,
            'normal_range_plasma': (150, 200),  # mg/dL
            'normal_range_units': 'mg/dL',
            'alternative_units': {'mM': (3.9, 5.2)},
            'pathways': ['cholesterol_biosynthesis', 'bile_acid_synthesis'],
            'clinical_significance': ['cardiovascular_disease', 'atherosclerosis'],
            'analytical_methods': ['enzymatic_assay', 'LC-MS/MS'],
            'synonyms': ['cholest-5-en-3β-ol'],
            'hmdb_id': 'HMDB0000067',
            'kegg_id': 'C00187'
        },
        'creatinine': {
            'molecular_formula': 'C4H7N3O',
            'molecular_weight': 113.12,
            'normal_range_plasma': (0.6, 1.2),  # mg/dL
            'normal_range_units': 'mg/dL',
            'alternative_units': {'μM': (53, 106)},
            'pathways': ['creatine_metabolism'],
            'clinical_significance': ['kidney_function', 'muscle_disorders'],
            'analytical_methods': ['colorimetric_assay', 'LC-MS/MS'],
            'synonyms': ['1-methylhydantoin-2-imino'],
            'hmdb_id': 'HMDB0000562',
            'kegg_id': 'C00791'
        }
    }
    
    DISEASE_KNOWLEDGE_BASE = {
        'diabetes': {
            'full_name': 'diabetes_mellitus',
            'icd10_codes': ['E10', 'E11', 'E12', 'E13', 'E14'],
            'key_biomarkers': ['glucose', 'HbA1c', 'insulin', 'C-peptide'],
            'altered_metabolites': {
                'elevated': ['glucose', 'lactate', 'ketone_bodies'],
                'decreased': ['insulin_sensitivity_markers']
            },
            'diagnostic_criteria': {
                'fasting_glucose': '>= 126 mg/dL',
                'hba1c': '>= 6.5%',
                'ogtt_2h': '>= 200 mg/dL'
            },
            'complications': ['diabetic_nephropathy', 'retinopathy', 'neuropathy', 'cardiovascular_disease'],
            'treatment_options': ['metformin', 'insulin', 'lifestyle_modification']
        },
        'cardiovascular_disease': {
            'full_name': 'cardiovascular_disease',
            'icd10_codes': ['I20', 'I21', 'I22', 'I25'],
            'key_biomarkers': ['cholesterol', 'LDL', 'HDL', 'triglycerides', 'CRP', 'troponin'],
            'altered_metabolites': {
                'elevated': ['cholesterol', 'TMAO', 'inflammatory_markers'],
                'decreased': ['HDL_cholesterol']
            },
            'risk_factors': ['hypertension', 'diabetes', 'smoking', 'obesity'],
            'diagnostic_tests': ['ECG', 'cardiac_enzymes', 'stress_test'],
            'treatment_options': ['statins', 'ACE_inhibitors', 'lifestyle_modification']
        }
    }
    
    PATHWAY_KNOWLEDGE_BASE = {
        'glycolysis': {
            'full_name': 'glycolytic_pathway',
            'kegg_id': 'hsa00010',
            'cellular_location': 'cytoplasm',
            'key_enzymes': ['hexokinase', 'phosphofructokinase', 'pyruvate_kinase'],
            'substrates': ['glucose'],
            'products': ['pyruvate', 'ATP', 'NADH'],
            'regulation': ['allosteric', 'transcriptional'],
            'clinical_relevance': ['diabetes', 'cancer_metabolism', 'exercise_physiology']
        },
        'tca_cycle': {
            'full_name': 'tricarboxylic_acid_cycle',
            'kegg_id': 'hsa00020',
            'cellular_location': 'mitochondria',
            'key_enzymes': ['citrate_synthase', 'isocitrate_dehydrogenase', 'α-ketoglutarate_dehydrogenase'],
            'substrates': ['acetyl-CoA'],
            'products': ['NADH', 'FADH2', 'ATP', 'CO2'],
            'regulation': ['allosteric', 'covalent_modification'],
            'clinical_relevance': ['metabolic_disorders', 'neurodegenerative_diseases']
        }
    }
    
    def __init__(self):
        """Initialize the validator with knowledge bases."""
        self.validation_history: List[ValidationResult] = []
        self.false_positive_patterns = self._load_false_positive_patterns()
        self.critical_error_patterns = self._load_critical_error_patterns()
    
    def validate_metabolite_information(self, content: str, metabolite_name: str) -> List[ValidationResult]:
        """Validate metabolite-specific information."""
        validations = []
        
        if metabolite_name.lower() not in self.METABOLITE_KNOWLEDGE_BASE:
            validations.append(ValidationResult(
                validation_id=f"metabolite_unknown_{int(time.time())}",
                validation_type=ValidationType.BIOMEDICAL_ACCURACY,
                validation_level=ValidationLevel.MEDIUM,
                passed=False,
                confidence=0.8,
                message=f"Unknown metabolite: {metabolite_name}",
                details={'metabolite': metabolite_name},
                suggestions=[f"Verify metabolite name: {metabolite_name}"]
            ))
            return validations
        
        metabolite_data = self.METABOLITE_KNOWLEDGE_BASE[metabolite_name.lower()]
        content_lower = content.lower()
        
        # Validate molecular formula
        if 'molecular formula' in content_lower or 'formula' in content_lower:
            expected_formula = metabolite_data['molecular_formula']
            if expected_formula not in content:
                validations.append(ValidationResult(
                    validation_id=f"formula_check_{int(time.time())}",
                    validation_type=ValidationType.FACTUAL_CONSISTENCY,
                    validation_level=ValidationLevel.HIGH,
                    passed=False,
                    confidence=0.9,
                    message=f"Incorrect or missing molecular formula for {metabolite_name}",
                    details={
                        'expected_formula': expected_formula,
                        'metabolite': metabolite_name
                    },
                    suggestions=[f"Include correct molecular formula: {expected_formula}"]
                ))
        
        # Validate molecular weight
        molecular_weight_pattern = r'molecular weight[:\s]*(\d+\.?\d*)'
        weight_match = re.search(molecular_weight_pattern, content_lower)
        if weight_match:
            stated_weight = float(weight_match.group(1))
            expected_weight = metabolite_data['molecular_weight']
            weight_tolerance = 0.1  # Allow 0.1 Da tolerance
            
            if abs(stated_weight - expected_weight) > weight_tolerance:
                validations.append(ValidationResult(
                    validation_id=f"weight_check_{int(time.time())}",
                    validation_type=ValidationType.FACTUAL_CONSISTENCY,
                    validation_level=ValidationLevel.HIGH,
                    passed=False,
                    confidence=0.95,
                    message=f"Incorrect molecular weight for {metabolite_name}",
                    details={
                        'stated_weight': stated_weight,
                        'expected_weight': expected_weight,
                        'tolerance': weight_tolerance
                    },
                    suggestions=[f"Use correct molecular weight: {expected_weight} Da"]
                ))
        
        # Validate reference ranges
        if any(term in content_lower for term in ['normal', 'range', 'reference', 'typical']):
            self._validate_reference_ranges(content, metabolite_name, metabolite_data, validations)
        
        # Validate pathway associations
        mentioned_pathways = []
        for pathway in metabolite_data['pathways']:
            if pathway.replace('_', ' ') in content_lower:
                mentioned_pathways.append(pathway)
        
        if mentioned_pathways:
            for pathway in mentioned_pathways:
                if pathway not in self.PATHWAY_KNOWLEDGE_BASE:
                    continue
                # Validate pathway-metabolite association
                pathway_data = self.PATHWAY_KNOWLEDGE_BASE[pathway]
                if metabolite_name.lower() not in str(pathway_data).lower():
                    validations.append(ValidationResult(
                        validation_id=f"pathway_association_{int(time.time())}",
                        validation_type=ValidationType.BIOMEDICAL_ACCURACY,
                        validation_level=ValidationLevel.MEDIUM,
                        passed=True,  # Assume correct unless proven otherwise
                        confidence=0.7,
                        message=f"Pathway association validated: {metabolite_name} - {pathway}",
                        details={'metabolite': metabolite_name, 'pathway': pathway}
                    ))
        
        return validations
    
    def validate_clinical_claims(self, content: str) -> List[ValidationResult]:
        """Validate clinical claims for safety and accuracy."""
        validations = []
        content_lower = content.lower()
        
        # Check for dangerous clinical advice
        dangerous_patterns = [
            r'stop taking.*medication',
            r'discontinue.*treatment',
            r'replace.*with',
            r'cure.*cancer',
            r'guaranteed.*treatment',
            r'miracle.*cure'
        ]
        
        for pattern in dangerous_patterns:
            if re.search(pattern, content_lower):
                validations.append(ValidationResult(
                    validation_id=f"dangerous_advice_{int(time.time())}",
                    validation_type=ValidationType.CLINICAL_SAFETY,
                    validation_level=ValidationLevel.CRITICAL,
                    passed=False,
                    confidence=0.95,
                    message="Content contains potentially dangerous clinical advice",
                    details={'pattern': pattern},
                    evidence=[f"Matched pattern: {pattern}"],
                    suggestions=["Remove or rephrase dangerous clinical advice", "Add appropriate disclaimers"]
                ))
        
        # Check for unsupported therapeutic claims
        therapeutic_claims = [
            r'treats?.*\b(cancer|diabetes|heart disease)\b',
            r'cures?.*\b(disease|condition)\b',
            r'prevents?.*\b(cancer|diabetes)\b'
        ]
        
        for pattern in therapeutic_claims:
            matches = re.finditer(pattern, content_lower)
            for match in matches:
                # Check if claim is properly qualified
                surrounding_text = content_lower[max(0, match.start()-50):match.end()+50]
                qualifiers = ['may', 'might', 'could', 'potential', 'studies suggest', 'preliminary']
                
                if not any(qualifier in surrounding_text for qualifier in qualifiers):
                    validations.append(ValidationResult(
                        validation_id=f"therapeutic_claim_{int(time.time())}",
                        validation_type=ValidationType.CLINICAL_SAFETY,
                        validation_level=ValidationLevel.HIGH,
                        passed=False,
                        confidence=0.8,
                        message="Unqualified therapeutic claim detected",
                        details={'claim': match.group()},
                        suggestions=["Add appropriate qualifiers (may, might, studies suggest)", "Include disclaimers"]
                    ))
        
        # Validate dosage information
        dosage_pattern = r'(\d+(?:\.\d+)?)\s*(mg|g|μg|ng)(?:/day|/kg|daily)?'
        dosage_matches = re.finditer(dosage_pattern, content_lower)
        
        for match in dosage_matches:
            dose = float(match.group(1))
            unit = match.group(2)
            
            # Check for unrealistic dosages
            if unit == 'g' and dose > 10:
                validations.append(ValidationResult(
                    validation_id=f"dosage_check_{int(time.time())}",
                    validation_type=ValidationType.CLINICAL_SAFETY,
                    validation_level=ValidationLevel.HIGH,
                    passed=False,
                    confidence=0.85,
                    message=f"Potentially dangerous dosage: {dose} {unit}",
                    details={'dose': dose, 'unit': unit},
                    suggestions=["Verify dosage is within safe therapeutic range"]
                ))
        
        return validations
    
    def validate_statistical_information(self, content: str) -> List[ValidationResult]:
        """Validate statistical claims and numerical data."""
        validations = []
        content_lower = content.lower()
        
        # Validate p-values
        p_value_pattern = r'p[<>=\s]*(\d*\.?\d+)'
        p_matches = re.finditer(p_value_pattern, content_lower)
        
        for match in p_matches:
            try:
                p_value = float(match.group(1))
                
                if p_value > 1.0:
                    validations.append(ValidationResult(
                        validation_id=f"p_value_invalid_{int(time.time())}",
                        validation_type=ValidationType.NUMERICAL_VALIDATION,
                        validation_level=ValidationLevel.HIGH,
                        passed=False,
                        confidence=1.0,
                        message=f"Invalid p-value: {p_value} (must be ≤ 1.0)",
                        details={'p_value': p_value},
                        suggestions=["Correct p-value to be between 0 and 1"]
                    ))
                elif p_value == 0.0:
                    validations.append(ValidationResult(
                        validation_id=f"p_value_zero_{int(time.time())}",
                        validation_type=ValidationType.SCIENTIFIC_VALIDITY,
                        validation_level=ValidationLevel.MEDIUM,
                        passed=False,
                        confidence=0.8,
                        message="P-value of exactly 0.0 is statistically implausible",
                        details={'p_value': p_value},
                        suggestions=["Use p < 0.001 instead of p = 0.0"]
                    ))
            except ValueError:
                continue
        
        # Validate percentage values
        percentage_pattern = r'(\d+(?:\.\d+)?)\s*%'
        percentage_matches = re.finditer(percentage_pattern, content)
        
        for match in percentage_matches:
            try:
                percentage = float(match.group(1))
                
                # Check for impossible percentages
                if percentage > 100:
                    # Check context - might be fold change
                    surrounding = content[max(0, match.start()-30):match.end()+30].lower()
                    if not any(term in surrounding for term in ['fold', 'increase', 'higher', 'change']):
                        validations.append(ValidationResult(
                            validation_id=f"percentage_invalid_{int(time.time())}",
                            validation_type=ValidationType.NUMERICAL_VALIDATION,
                            validation_level=ValidationLevel.MEDIUM,
                            passed=False,
                            confidence=0.9,
                            message=f"Percentage > 100% without clear context: {percentage}%",
                            details={'percentage': percentage},
                            suggestions=["Clarify if this is a percentage or fold change"]
                        ))
            except ValueError:
                continue
        
        # Validate correlation coefficients
        correlation_pattern = r'r\s*[=:]\s*([+-]?\d*\.?\d+)'
        corr_matches = re.finditer(correlation_pattern, content_lower)
        
        for match in corr_matches:
            try:
                correlation = float(match.group(1))
                
                if abs(correlation) > 1.0:
                    validations.append(ValidationResult(
                        validation_id=f"correlation_invalid_{int(time.time())}",
                        validation_type=ValidationType.NUMERICAL_VALIDATION,
                        validation_level=ValidationLevel.HIGH,
                        passed=False,
                        confidence=1.0,
                        message=f"Invalid correlation coefficient: {correlation} (must be between -1 and 1)",
                        details={'correlation': correlation},
                        suggestions=["Correct correlation coefficient to be between -1 and 1"]
                    ))
            except ValueError:
                continue
        
        return validations
    
    def validate_terminology_consistency(self, content: str) -> List[ValidationResult]:
        """Validate consistent use of scientific terminology."""
        validations = []
        
        # Check for consistent metabolite naming
        metabolite_variants = {
            'glucose': ['glucose', 'd-glucose', 'dextrose', 'blood sugar'],
            'lactate': ['lactate', 'lactic acid', 'milk acid'],
            'cholesterol': ['cholesterol', 'cholest-5-en-3β-ol']
        }
        
        for canonical_name, variants in metabolite_variants.items():
            found_variants = []
            for variant in variants:
                if variant.lower() in content.lower():
                    found_variants.append(variant)
            
            if len(found_variants) > 1:
                # Check if usage is consistent within context
                validations.append(ValidationResult(
                    validation_id=f"terminology_consistency_{int(time.time())}",
                    validation_type=ValidationType.TERMINOLOGY_CONSISTENCY,
                    validation_level=ValidationLevel.LOW,
                    passed=True,  # Not necessarily wrong, but worth noting
                    confidence=0.6,
                    message=f"Multiple terms used for {canonical_name}: {found_variants}",
                    details={'canonical_name': canonical_name, 'variants_found': found_variants},
                    suggestions=["Consider using consistent terminology throughout"]
                ))
        
        return validations
    
    def _validate_reference_ranges(self, content: str, metabolite_name: str, 
                                 metabolite_data: Dict[str, Any], validations: List[ValidationResult]):
        """Validate reference ranges for metabolites."""
        content_lower = content.lower()
        
        # Extract numeric ranges from content
        range_patterns = [
            r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)',  # 3.9-6.1
            r'(\d+\.?\d*)\s*to\s*(\d+\.?\d*)',  # 3.9 to 6.1
            r'between\s*(\d+\.?\d*)\s*and\s*(\d+\.?\d*)'  # between 3.9 and 6.1
        ]
        
        found_ranges = []
        for pattern in range_patterns:
            matches = re.finditer(pattern, content_lower)
            for match in matches:
                try:
                    lower = float(match.group(1))
                    upper = float(match.group(2))
                    found_ranges.append((lower, upper))
                except ValueError:
                    continue
        
        if found_ranges:
            expected_range = metabolite_data['normal_range_plasma']
            expected_lower, expected_upper = expected_range
            
            for found_lower, found_upper in found_ranges:
                # Allow some tolerance for reference ranges (different populations/methods)
                tolerance = 0.2  # 20% tolerance
                
                if (abs(found_lower - expected_lower) / expected_lower > tolerance or
                    abs(found_upper - expected_upper) / expected_upper > tolerance):
                    
                    validations.append(ValidationResult(
                        validation_id=f"reference_range_{int(time.time())}",
                        validation_type=ValidationType.FACTUAL_CONSISTENCY,
                        validation_level=ValidationLevel.MEDIUM,
                        passed=False,
                        confidence=0.8,
                        message=f"Reference range deviation for {metabolite_name}",
                        details={
                            'found_range': (found_lower, found_upper),
                            'expected_range': expected_range,
                            'deviation': 'significant'
                        },
                        suggestions=[
                            f"Expected range: {expected_lower}-{expected_upper} {metabolite_data['normal_range_units']}",
                            "Verify reference range source and population"
                        ]
                    ))
    
    def _load_false_positive_patterns(self) -> List[str]:
        """Load patterns that commonly cause false positive validations."""
        return [
            r'studies? suggest',
            r'may be associated',
            r'preliminary evidence',
            r'further research needed',
            r'potential biomarker'
        ]
    
    def _load_critical_error_patterns(self) -> List[str]:
        """Load patterns that indicate critical errors."""
        return [
            r'cure.*cancer',
            r'replace.*prescription',
            r'stop.*medication',
            r'guaranteed.*results',
            r'miracle.*treatment'
        ]
    
    def validate_content(self, content: str, context: Optional[Dict[str, Any]] = None) -> ValidationReport:
        """Perform comprehensive content validation."""
        all_validations = []
        
        # Extract mentioned metabolites and validate
        mentioned_metabolites = self._extract_metabolites_from_content(content)
        for metabolite in mentioned_metabolites:
            metabolite_validations = self.validate_metabolite_information(content, metabolite)
            all_validations.extend(metabolite_validations)
        
        # Validate clinical claims
        clinical_validations = self.validate_clinical_claims(content)
        all_validations.extend(clinical_validations)
        
        # Validate statistical information
        statistical_validations = self.validate_statistical_information(content)
        all_validations.extend(statistical_validations)
        
        # Validate terminology consistency
        terminology_validations = self.validate_terminology_consistency(content)
        all_validations.extend(terminology_validations)
        
        # Calculate summary statistics
        total_validations = len(all_validations)
        passed_validations = sum(1 for v in all_validations if v.passed)
        failed_validations = total_validations - passed_validations
        critical_issues = sum(1 for v in all_validations 
                            if v.validation_level == ValidationLevel.CRITICAL and not v.passed)
        warnings = sum(1 for v in all_validations 
                      if v.validation_level in [ValidationLevel.HIGH, ValidationLevel.MEDIUM] and not v.passed)
        
        # Calculate overall score
        if total_validations == 0:
            overall_score = 1.0
        else:
            # Weight by validation level
            level_weights = {
                ValidationLevel.CRITICAL: 10,
                ValidationLevel.HIGH: 5,
                ValidationLevel.MEDIUM: 2,
                ValidationLevel.LOW: 1,
                ValidationLevel.INFO: 0.5
            }
            
            total_weight = sum(level_weights[v.validation_level] for v in all_validations)
            passed_weight = sum(level_weights[v.validation_level] for v in all_validations if v.passed)
            overall_score = passed_weight / total_weight if total_weight > 0 else 1.0
        
        # Generate recommendations
        recommendations = self._generate_recommendations(all_validations)
        
        # Create summary
        summary = {
            'validation_categories': {
                vtype.value: sum(1 for v in all_validations if v.validation_type == vtype)
                for vtype in ValidationType
            },
            'validation_levels': {
                level.value: sum(1 for v in all_validations if v.validation_level == level)
                for level in ValidationLevel
            },
            'content_length': len(content),
            'metabolites_mentioned': mentioned_metabolites,
            'validation_timestamp': time.time()
        }
        
        report_id = f"validation_report_{int(time.time())}"
        
        return ValidationReport(
            report_id=report_id,
            content_analyzed=content[:200] + "..." if len(content) > 200 else content,
            total_validations=total_validations,
            passed_validations=passed_validations,
            failed_validations=failed_validations,
            critical_issues=critical_issues,
            warnings=warnings,
            overall_score=overall_score,
            validation_results=all_validations,
            summary=summary,
            recommendations=recommendations
        )
    
    def _extract_metabolites_from_content(self, content: str) -> List[str]:
        """Extract mentioned metabolites from content."""
        metabolites = []
        content_lower = content.lower()
        
        for metabolite_name in self.METABOLITE_KNOWLEDGE_BASE.keys():
            if metabolite_name in content_lower:
                metabolites.append(metabolite_name)
                
            # Check synonyms
            metabolite_data = self.METABOLITE_KNOWLEDGE_BASE[metabolite_name]
            for synonym in metabolite_data.get('synonyms', []):
                if synonym.lower() in content_lower and metabolite_name not in metabolites:
                    metabolites.append(metabolite_name)
                    break
        
        return metabolites
    
    def _generate_recommendations(self, validations: List[ValidationResult]) -> List[str]:
        """Generate actionable recommendations based on validation results."""
        recommendations = []
        
        # Critical issues first
        critical_validations = [v for v in validations 
                              if v.validation_level == ValidationLevel.CRITICAL and not v.passed]
        
        if critical_validations:
            recommendations.append("URGENT: Address critical safety issues before clinical use")
            for validation in critical_validations[:3]:  # Top 3 critical issues
                recommendations.extend(validation.suggestions)
        
        # High priority issues
        high_priority = [v for v in validations 
                        if v.validation_level == ValidationLevel.HIGH and not v.passed]
        
        if high_priority:
            recommendations.append("High Priority: Verify factual accuracy")
            unique_suggestions = set()
            for validation in high_priority[:5]:  # Top 5 high priority
                unique_suggestions.update(validation.suggestions)
            recommendations.extend(list(unique_suggestions)[:3])
        
        # General recommendations
        if len([v for v in validations if not v.passed]) > len(validations) * 0.2:
            recommendations.append("Consider expert review of biomedical content")
        
        if any('reference' in v.validation_type.value for v in validations if not v.passed):
            recommendations.append("Add authoritative references to support claims")
        
        return recommendations[:10]  # Limit to top 10 recommendations


class ResponseQualityAssessor:
    """
    Assesses overall response quality including completeness, relevance, and clarity.
    """
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': 0.0,
            'relevance': 0.0,
            'clarity': 0.0,
            'accuracy': 0.0,
            'actionability': 0.0,
            'safety': 0.0
        }
    
    def assess_response_quality(self, 
                              query: str,
                              response: str,
                              validation_report: ValidationReport) -> Dict[str, Any]:
        """Assess comprehensive response quality."""
        
        # Calculate individual metrics
        completeness = self._assess_completeness(query, response)
        relevance = self._assess_relevance(query, response)
        clarity = self._assess_clarity(response)
        accuracy = validation_report.overall_score
        actionability = self._assess_actionability(response)
        safety = 1.0 if validation_report.critical_issues == 0 else max(0.0, 1.0 - validation_report.critical_issues * 0.2)
        
        # Calculate overall quality score
        weights = {
            'completeness': 0.15,
            'relevance': 0.20,
            'clarity': 0.15,
            'accuracy': 0.25,
            'actionability': 0.10,
            'safety': 0.15
        }
        
        overall_score = (
            completeness * weights['completeness'] +
            relevance * weights['relevance'] +
            clarity * weights['clarity'] +
            accuracy * weights['accuracy'] +
            actionability * weights['actionability'] +
            safety * weights['safety']
        )
        
        # Generate quality assessment
        quality_level = self._determine_quality_level(overall_score)
        
        return {
            'overall_score': overall_score,
            'quality_level': quality_level,
            'metrics': {
                'completeness': completeness,
                'relevance': relevance,
                'clarity': clarity,
                'accuracy': accuracy,
                'actionability': actionability,
                'safety': safety
            },
            'assessment_details': {
                'strengths': self._identify_strengths(overall_score, {
                    'completeness': completeness,
                    'relevance': relevance,
                    'clarity': clarity,
                    'accuracy': accuracy,
                    'actionability': actionability,
                    'safety': safety
                }),
                'areas_for_improvement': self._identify_improvements(overall_score, {
                    'completeness': completeness,
                    'relevance': relevance,
                    'clarity': clarity,
                    'accuracy': accuracy,
                    'actionability': actionability,
                    'safety': safety
                })
            }
        }
    
    def _assess_completeness(self, query: str, response: str) -> float:
        """Assess how completely the response addresses the query."""
        query_lower = query.lower()
        response_lower = response.lower()
        
        # Extract key question words and topics
        question_words = ['what', 'how', 'why', 'when', 'where', 'which']
        question_word_coverage = sum(1 for word in question_words if word in query_lower) / len(question_words)
        
        # Basic length check
        length_score = min(1.0, len(response) / 500)  # Optimal around 500 chars
        
        # Check for comprehensive coverage
        key_terms = re.findall(r'\b\w{4,}\b', query_lower)
        covered_terms = sum(1 for term in key_terms if term in response_lower) / max(1, len(key_terms))
        
        return np.mean([question_word_coverage, length_score, covered_terms])
    
    def _assess_relevance(self, query: str, response: str) -> float:
        """Assess relevance of response to query."""
        query_terms = set(re.findall(r'\b\w{4,}\b', query.lower()))
        response_terms = set(re.findall(r'\b\w{4,}\b', response.lower()))
        
        if not query_terms:
            return 1.0
        
        overlap = len(query_terms.intersection(response_terms))
        relevance_score = overlap / len(query_terms)
        
        return min(1.0, relevance_score)
    
    def _assess_clarity(self, response: str) -> float:
        """Assess clarity and readability of response."""
        # Simple readability metrics
        sentences = re.split(r'[.!?]+', response)
        if not sentences:
            return 0.0
        
        avg_sentence_length = np.mean([len(sentence.split()) for sentence in sentences if sentence.strip()])
        
        # Optimal sentence length around 15-20 words
        length_score = 1.0 - abs(avg_sentence_length - 17.5) / 17.5 if avg_sentence_length > 0 else 0.0
        length_score = max(0.0, min(1.0, length_score))
        
        # Check for technical jargon balance
        technical_terms = sum(1 for word in response.split() 
                            if any(term in word.lower() for term in ['metabol', 'clinic', 'biomark', 'pathw']))
        
        technical_density = technical_terms / len(response.split()) if response.split() else 0
        technical_score = 1.0 if 0.05 <= technical_density <= 0.3 else 0.5  # Good balance
        
        return np.mean([length_score, technical_score])
    
    def _assess_actionability(self, response: str) -> float:
        """Assess how actionable the response is."""
        actionable_phrases = [
            'you can', 'you should', 'consider', 'recommended', 'next step',
            'consult', 'test for', 'measure', 'analyze', 'investigate'
        ]
        
        response_lower = response.lower()
        actionable_count = sum(1 for phrase in actionable_phrases if phrase in response_lower)
        
        # Normalize by response length
        actionability_score = min(1.0, actionable_count / 3)  # Optimal around 3 actionable items
        
        return actionability_score
    
    def _determine_quality_level(self, score: float) -> str:
        """Determine quality level based on overall score."""
        if score >= 0.9:
            return 'excellent'
        elif score >= 0.8:
            return 'good'
        elif score >= 0.7:
            return 'satisfactory'
        elif score >= 0.6:
            return 'needs_improvement'
        else:
            return 'poor'
    
    def _identify_strengths(self, overall_score: float, metrics: Dict[str, float]) -> List[str]:
        """Identify response strengths."""
        strengths = []
        
        if metrics['safety'] >= 0.9:
            strengths.append("Clinically safe content")
        
        if metrics['accuracy'] >= 0.8:
            strengths.append("Factually accurate information")
        
        if metrics['relevance'] >= 0.8:
            strengths.append("Highly relevant to query")
        
        if metrics['clarity'] >= 0.8:
            strengths.append("Clear and well-structured")
        
        if metrics['completeness'] >= 0.8:
            strengths.append("Comprehensive coverage")
        
        return strengths
    
    def _identify_improvements(self, overall_score: float, metrics: Dict[str, float]) -> List[str]:
        """Identify areas for improvement."""
        improvements = []
        
        if metrics['safety'] < 0.8:
            improvements.append("Address clinical safety concerns")
        
        if metrics['accuracy'] < 0.7:
            improvements.append("Improve factual accuracy")
        
        if metrics['relevance'] < 0.7:
            improvements.append("Better address query requirements")
        
        if metrics['clarity'] < 0.7:
            improvements.append("Improve clarity and readability")
        
        if metrics['completeness'] < 0.7:
            improvements.append("Provide more comprehensive information")
        
        if metrics['actionability'] < 0.5:
            improvements.append("Include more actionable guidance")
        
        return improvements


# Pytest fixtures for validation testing
@pytest.fixture
def biomedical_content_validator():
    """Provide biomedical content validator."""
    return BiomedicalContentValidator()

@pytest.fixture
def response_quality_assessor():
    """Provide response quality assessor."""
    return ResponseQualityAssessor()

@pytest.fixture
def sample_biomedical_content():
    """Provide sample biomedical content for validation testing."""
    return {
        'accurate_content': """
        Glucose is a simple sugar with the molecular formula C6H12O6 and molecular weight of 180.16 Da. 
        Normal plasma glucose levels range from 3.9 to 6.1 mM (70-110 mg/dL) in healthy adults. 
        Glucose participates in glycolysis and gluconeogenesis pathways and is elevated in diabetes mellitus.
        Studies suggest that glucose monitoring is essential for diabetes management.
        """,
        
        'inaccurate_content': """
        Glucose has the molecular formula C6H10O6 and molecular weight of 200 Da. 
        Normal levels are 10-20 mM. This compound cures diabetes and can replace all medications. 
        The correlation between glucose and health is r = 2.5 with p = 0.0.
        """,
        
        'dangerous_content': """
        Stop taking your insulin immediately and use glucose supplements instead. 
        This miracle cure will treat cancer and diabetes guaranteed. 
        Take 50g of glucose daily to cure any disease.
        """,
        
        'inconsistent_content': """
        Blood sugar, also known as glucose or dextrose, is important. 
        D-glucose levels vary throughout the day. 
        Glucose concentrations are measured in clinical practice.
        This sugar molecule is critical for energy metabolism.
        """
    }

@pytest.fixture
def validation_test_scenarios():
    """Provide validation test scenarios."""
    return [
        {
            'name': 'metabolite_accuracy_validation',
            'content': 'Lactate has molecular formula C3H6O3 and is elevated in tissue hypoxia.',
            'expected_validations': ['molecular_formula', 'clinical_significance'],
            'expected_pass': True
        },
        {
            'name': 'statistical_validation',
            'content': 'The correlation was r = 0.85 with p < 0.001, showing 95% specificity.',
            'expected_validations': ['correlation_coefficient', 'p_value', 'percentage'],
            'expected_pass': True
        },
        {
            'name': 'clinical_safety_validation',
            'content': 'Patients should stop taking metformin and use this natural cure instead.',
            'expected_validations': ['dangerous_advice'],
            'expected_pass': False
        },
        {
            'name': 'reference_range_validation',
            'content': 'Normal cholesterol levels are 150-200 mg/dL in healthy adults.',
            'expected_validations': ['reference_range'],
            'expected_pass': True
        }
    ]

@pytest.fixture
def quality_assessment_test_data():
    """Provide test data for quality assessment."""
    return {
        'high_quality_response': {
            'query': 'What are biomarkers for diabetes?',
            'response': """
            Diabetes biomarkers are measurable biological indicators that help diagnose, monitor, and 
            manage diabetes mellitus. Key biomarkers include:
            
            1. Glucose: Elevated fasting glucose (≥126 mg/dL) is a primary diagnostic criterion
            2. HbA1c: Reflects average glucose levels over 2-3 months (≥6.5% indicates diabetes)
            3. C-peptide: Measures endogenous insulin production
            4. Ketone bodies: Elevated in diabetic ketoacidosis
            
            These biomarkers help clinicians assess disease progression and treatment effectiveness. 
            Regular monitoring is recommended for optimal diabetes management.
            """
        },
        'poor_quality_response': {
            'query': 'What are biomarkers for diabetes?',
            'response': 'Sugar is bad. Check it.'
        },
        'unsafe_response': {
            'query': 'How to manage diabetes?',
            'response': 'Stop taking insulin and eat more sugar. This will cure diabetes completely.'
        }
    }

@pytest.fixture
def validation_report_samples():
    """Provide sample validation reports."""
    validator = BiomedicalContentValidator()
    
    samples = {}
    
    # Generate reports for different content types
    content_samples = {
        'accurate': "Glucose (C6H12O6) levels are normally 3.9-6.1 mM in plasma",
        'inaccurate': "Glucose (C6H10O5) levels are 50-100 mM normally",
        'dangerous': "Stop taking medication and use glucose to cure cancer"
    }
    
    for content_type, content in content_samples.items():
        report = validator.validate_content(content)
        samples[content_type] = report
    
    return samples

@pytest.fixture
def comprehensive_validation_suite():
    """Provide comprehensive validation test suite."""
    return {
        'validator': BiomedicalContentValidator(),
        'assessor': ResponseQualityAssessor(),
        'test_cases': [
            {
                'category': 'metabolite_validation',
                'tests': [
                    {
                        'content': 'Lactate molecular formula is C3H6O3',
                        'metabolite': 'lactate',
                        'expected_pass': True
                    },
                    {
                        'content': 'Lactate molecular formula is C3H8O3',
                        'metabolite': 'lactate',
                        'expected_pass': False
                    }
                ]
            },
            {
                'category': 'clinical_safety',
                'tests': [
                    {
                        'content': 'Studies suggest metabolomics may help in diagnosis',
                        'expected_pass': True
                    },
                    {
                        'content': 'Stop taking all medications immediately',
                        'expected_pass': False
                    }
                ]
            },
            {
                'category': 'statistical_validation',
                'tests': [
                    {
                        'content': 'Correlation r = 0.75, p < 0.01',
                        'expected_pass': True
                    },
                    {
                        'content': 'Correlation r = 2.5, p = 0.0',
                        'expected_pass': False
                    }
                ]
            }
        ]
    }