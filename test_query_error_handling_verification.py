#!/usr/bin/env python3
"""
Query Error Handling Verification Test for CMO-LIGHTRAG-007-T04

This script tests and verifies that error handling for query failures and API limits
is properly implemented with the new QueryParam configuration.

Test Coverage:
1. QueryParam validation and creation errors
2. LightRAG API call error handling
3. Rate limiting and circuit breaker integration
4. API limit handling (budget, token limits)
5. Proper error propagation and logging

Author: Claude Code (Anthropic)
Created: 2025-08-07
"""

import asyncio
import pytest
import sys
import tempfile
import json
import time
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch, MagicMock

# Add project path to allow imports
sys.path.append(str(Path(__file__).parent))

# Test imports
try:
    from lightrag_integration.clinical_metabolomics_rag import (
        ClinicalMetabolomicsRAG, ClinicalMetabolomicsRAGError,
        LightRAGConfig, CircuitBreakerError
    )
    IMPORTS_AVAILABLE = True
except ImportError as e:
    print(f"Import error: {e}")
    IMPORTS_AVAILABLE = False

try:
    from lightrag import QueryParam
    LIGHTRAG_AVAILABLE = True
except ImportError:
    LIGHTRAG_AVAILABLE = False
    # Mock QueryParam for testing
    class QueryParam:
        def __init__(self, **kwargs):
            self.mode = kwargs.get('mode', 'hybrid')
            self.response_type = kwargs.get('response_type', 'Multiple Paragraphs')
            self.top_k = kwargs.get('top_k', 10)
            self.max_total_tokens = kwargs.get('max_total_tokens', 8000)
            for k, v in kwargs.items():
                setattr(self, k, v)


class TestQueryErrorHandling:
    """Test suite for query error handling with QueryParam integration."""
    
    @pytest.fixture
    def temp_dir(self):
        """Create a temporary directory for testing."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            yield Path(tmp_dir)
    
    @pytest.fixture  
    def valid_config(self, temp_dir):
        """Create a valid configuration for testing."""
        return LightRAGConfig(
            working_dir=temp_dir / "test_kb",
            api_key="test-key-123",
            model="gpt-4o-mini",
            max_tokens=1000,
            embedding_model="text-embedding-3-large",
            enable_cost_tracking=True
        )
    
    @pytest.mark.asyncio
    async def test_empty_query_validation(self, valid_config):
        """Test that empty queries are properly validated and rejected."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Test empty string
        with pytest.raises(ValueError, match="Query cannot be empty"):
            await rag.query("")
            
        # Test whitespace only
        with pytest.raises(ValueError, match="Query cannot be empty"):
            await rag.query("   \t\n   ")
            
        # Test None
        with pytest.raises(ValueError, match="Query cannot be empty"):
            await rag.query(None)
            
    @pytest.mark.asyncio
    async def test_uninitialized_rag_error(self, valid_config):
        """Test that uninitialized RAG system throws proper error."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        rag.is_initialized = False  # Force uninitialized state
        
        with pytest.raises(ClinicalMetabolomicsRAGError, match="RAG system not initialized"):
            await rag.query("Valid query")
    
    @pytest.mark.asyncio
    async def test_query_param_creation_error_handling(self, valid_config):
        """Test error handling when QueryParam creation fails."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Test with invalid mode
        with patch('lightrag_integration.clinical_metabolomics_rag.QueryParam') as mock_query_param:
            mock_query_param.side_effect = ValueError("Invalid mode parameter")
            
            with pytest.raises(ClinicalMetabolomicsRAGError, match="Query processing failed"):
                await rag.query("Test query", mode="invalid_mode")
                
    @pytest.mark.asyncio  
    async def test_lightrag_api_error_handling(self, valid_config):
        """Test error handling when LightRAG API call fails."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Mock LightRAG instance to raise API error
        mock_lightrag = AsyncMock()
        mock_lightrag.aquery.side_effect = Exception("LightRAG API failure")
        rag.lightrag_instance = mock_lightrag
        
        with pytest.raises(ClinicalMetabolomicsRAGError, match="Query processing failed"):
            await rag.query("Test query")
            
        # Verify the error was logged
        # Note: In a full test, we'd check the logger output
        
    @pytest.mark.asyncio
    async def test_circuit_breaker_integration(self, valid_config):
        """Test that circuit breaker errors are properly handled."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Simulate circuit breaker being open
        with patch.object(rag.llm_circuit_breaker, 'call', side_effect=CircuitBreakerError("Circuit breaker is open")):
            with pytest.raises(ClinicalMetabolomicsRAGError, match="LLM service temporarily unavailable"):
                # This would trigger circuit breaker during LLM operations
                await rag._get_llm_function()("test prompt")
    
    def test_budget_limit_checking(self, valid_config):
        """Test that budget limits are properly checked during query processing."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        # Set up config with budget limits
        valid_config.daily_budget_limit = 10.0
        valid_config.monthly_budget_limit = 100.0
        
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Test budget limit validation
        assert rag.config.daily_budget_limit == 10.0
        assert rag.config.monthly_budget_limit == 100.0
        
        # In a full implementation, we would test budget enforcement during queries
        
    def test_query_param_configuration_validation(self, valid_config):
        """Test that QueryParam is configured with proper biomedical defaults."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Test default biomedical configuration
        test_params = {
            'mode': 'hybrid',
            'response_type': 'Multiple Paragraphs',
            'top_k': 10,
            'max_total_tokens': 8000
        }
        
        # Verify parameters are correctly configured
        query_param = QueryParam(**test_params)
        assert query_param.mode == 'hybrid'
        assert query_param.response_type == 'Multiple Paragraphs'
        assert query_param.top_k == 10
        assert query_param.max_total_tokens == 8000
        
    def test_comprehensive_error_handling_coverage(self, valid_config):
        """Test that all error handling components are properly initialized."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Verify error handling components exist
        assert hasattr(rag, 'rate_limiter')
        assert hasattr(rag, 'request_queue') 
        assert hasattr(rag, 'llm_circuit_breaker')
        assert hasattr(rag, 'embedding_circuit_breaker')
        assert hasattr(rag, 'error_metrics')
        
        # Verify error metrics structure
        expected_metrics = [
            'rate_limit_events', 'circuit_breaker_trips', 'retry_attempts',
            'recovery_events', 'last_rate_limit', 'last_circuit_break',
            'api_call_stats'
        ]
        for metric in expected_metrics:
            assert metric in rag.error_metrics
            
    @pytest.mark.asyncio
    async def test_error_propagation_and_logging(self, valid_config):
        """Test that errors are properly propagated and logged."""
        if not IMPORTS_AVAILABLE:
            pytest.skip("Required imports not available")
            
        rag = ClinicalMetabolomicsRAG(config=valid_config)
        
        # Mock logger to capture error messages
        mock_logger = Mock()
        rag.logger = mock_logger
        
        # Mock LightRAG to raise specific error
        mock_lightrag = AsyncMock()
        mock_lightrag.aquery.side_effect = RuntimeError("Specific error message")
        rag.lightrag_instance = mock_lightrag
        
        # Test error propagation
        with pytest.raises(ClinicalMetabolomicsRAGError) as exc_info:
            await rag.query("Test query")
            
        # Verify error message contains original error
        assert "Query processing failed" in str(exc_info.value)
        
        # Verify error was logged
        mock_logger.error.assert_called()


async def run_verification_tests():
    """Run verification tests for query error handling."""
    print("=" * 60)
    print("QUERY ERROR HANDLING VERIFICATION")
    print("CMO-LIGHTRAG-007-T04 Completion Test")
    print("=" * 60)
    
    if not IMPORTS_AVAILABLE:
        print("‚ùå Required imports not available - cannot run full tests")
        return False
        
    # Create test config
    with tempfile.TemporaryDirectory() as tmp_dir:
        config = LightRAGConfig(
            working_dir=Path(tmp_dir) / "test_kb",
            api_key="test-key-123",
            model="gpt-4o-mini",
            max_tokens=1000,
            embedding_model="text-embedding-3-large",
            enable_cost_tracking=True
        )
        
        print("üîß Testing QueryParam error handling integration...")
        
        # Test 1: Basic error handling structure
        try:
            rag = ClinicalMetabolomicsRAG(config=config)
            
            # Verify error handling components
            error_components = [
                'rate_limiter', 'request_queue', 'llm_circuit_breaker',
                'embedding_circuit_breaker', 'error_metrics'
            ]
            
            missing_components = []
            for component in error_components:
                if not hasattr(rag, component):
                    missing_components.append(component)
                    
            if missing_components:
                print(f"‚ùå Missing error handling components: {missing_components}")
                return False
                
            print("‚úÖ Error handling components properly initialized")
            
        except Exception as e:
            print(f"‚ùå Error initializing RAG system: {e}")
            return False
            
        # Test 2: Query validation
        try:
            # Test empty query validation
            try:
                await rag.query("")
                print("‚ùå Empty query validation failed - should raise ValueError")
                return False
            except ValueError as e:
                if "Query cannot be empty" in str(e):
                    print("‚úÖ Empty query validation working properly")
                else:
                    print(f"‚ùå Unexpected error message: {e}")
                    return False
                    
        except Exception as e:
            print(f"‚ùå Query validation test failed: {e}")
            return False
            
        # Test 3: Uninitialized RAG error
        try:
            rag.is_initialized = False
            try:
                await rag.query("Valid query")
                print("‚ùå Uninitialized RAG error handling failed")
                return False
            except ClinicalMetabolomicsRAGError as e:
                if "RAG system not initialized" in str(e):
                    print("‚úÖ Uninitialized RAG error handling working properly")
                else:
                    print(f"‚ùå Unexpected error message: {e}")
                    return False
                    
        except Exception as e:
            print(f"‚ùå Uninitialized RAG test failed: {e}")
            return False
            
        # Test 4: QueryParam configuration
        try:
            query_param = QueryParam(
                mode='hybrid',
                response_type='Multiple Paragraphs',
                top_k=10,
                max_total_tokens=8000
            )
            
            # Verify biomedical defaults
            assert query_param.mode == 'hybrid'
            assert query_param.response_type == 'Multiple Paragraphs'
            assert query_param.top_k == 10
            assert query_param.max_total_tokens == 8000
            
            print("‚úÖ QueryParam configuration validation working properly")
            
        except Exception as e:
            print(f"‚ùå QueryParam configuration test failed: {e}")
            return False
            
        # Test 5: Error metrics structure
        try:
            expected_metrics = [
                'rate_limit_events', 'circuit_breaker_trips', 'retry_attempts',
                'recovery_events', 'api_call_stats'
            ]
            
            rag.is_initialized = True  # Reset for this test
            missing_metrics = []
            for metric in expected_metrics:
                if metric not in rag.error_metrics:
                    missing_metrics.append(metric)
                    
            if missing_metrics:
                print(f"‚ùå Missing error metrics: {missing_metrics}")
                return False
                
            print("‚úÖ Error metrics structure properly configured")
            
        except Exception as e:
            print(f"‚ùå Error metrics test failed: {e}")
            return False
            
        print("\n" + "=" * 60)
        print("‚úÖ ALL QUERY ERROR HANDLING VERIFICATION TESTS PASSED")
        print("‚úÖ CMO-LIGHTRAG-007-T04 READY FOR COMPLETION")
        print("=" * 60)
        
        return True


if __name__ == "__main__":
    print("Starting Query Error Handling Verification...")
    
    try:
        success = asyncio.run(run_verification_tests())
        if success:
            print("\nüéâ Query error handling verification completed successfully!")
            sys.exit(0)
        else:
            print("\n‚ùå Query error handling verification failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nüí• Verification test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)